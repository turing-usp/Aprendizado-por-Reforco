{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ReplayBuffer.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"xtKzzOaYrN7z","colab_type":"text"},"source":["# üîÅ Experience Replay\n","\n","Uma grande desvantagem das redes neurais √© a necessidade de treinar com uma grande quantidade de dados para obter um bom aprendizado. Isso torna seu uso em algoritmos \"online\" como os de Temporal Difference bem dif√≠cil, j√° que ela recebe apenas uma transi√ß√£o a cada instante de tempo para o treinamento.\n","\n","Entretanto, como Q-Learning √© um algoritmo off-policy, n√≥s podemos aproveitar as experi√™ncias anteriores do nosso agente para utilizar em um batch no treinamento da nossa rede. √â dessa ideia que surge o conceito do **Experience Replay**, um buffer para guardar todas as experi√™ncias passadas do nosso agente.\n","\n","Para entender como isso funciona, vamos relembrar da atualiza√ß√£o do Q-Learning:\n","\n","<img src=\"https://latex.codecogs.com/svg.latex?Q(S,&space;A)&space;\\leftarrow&space;Q(S,&space;A)&space;&plus;&space;\\alpha&space;[R&space;&plus;&space;\\gamma&space;\\max_{a}Q(S',&space;a)&space;-&space;Q(S,&space;A)]\" title=\"Q(S, A) \\leftarrow Q(S, A) + \\alpha [R + \\gamma \\max_{a}Q(S', a) - Q(S, A)]\" />\n","\n","Observe que para atualizar o valor *Q* de um par estado-a√ß√£o, precisamos saber apenas o estado *S*, a a√ß√£o tomada *A*, a recompensa recebida *R* e o estado seguinte *S'*.\n","\n","TODO: Continuar"]},{"cell_type":"code","metadata":{"id":"0pzu8HKRjX6l","colab_type":"code","colab":{}},"source":["import numpy as np\n","\n","class ReplayBuffer:\n","    \"\"\"Experience Replay Buffer para DQNs.\"\"\"\n","    def __init__(self, max_length, observation_space):\n","        \"\"\"Cria um Replay Buffer.\n","\n","        Par√¢metros\n","        ----------\n","        max_length: int\n","            Tamanho m√°ximo do Replay Buffer.\n","        observation_space: int\n","            Tamanho do espa√ßo de observa√ß√£o.\n","        \"\"\"\n","        self.index, self.size, self.max_length = 0, 0, max_length\n","\n","        self.states = np.zeros((max_length, observation_space), dtype=np.float32)\n","        self.actions = np.zeros((max_length), dtype=np.int32)\n","        self.rewards = np.zeros((max_length), dtype=np.float32)\n","        self.next_states = np.zeros((max_length, observation_space), dtype=np.float32)\n","        self.dones = np.zeros((max_length), dtype=np.float32)\n","\n","    def __len__(self):\n","        \"\"\"Retorna o tamanho do buffer.\"\"\"\n","        return self.size\n","\n","    def update(self, state, action, reward, next_state, done):\n","        \"\"\"Adiciona uma experi√™ncia ao Replay Buffer.\n","\n","        Par√¢metros\n","        ----------\n","        state: np.array\n","            Estado da transi√ß√£o.\n","        action: int\n","            A√ß√£o tomada.\n","        reward: float\n","            Recompensa recebida.\n","        state: np.array\n","            Estado seguinte.\n","        done: int\n","            Flag indicando se o epis√≥dio acabou.\n","        \"\"\"\n","        self.states[self.index] = state\n","        self.actions[self.index] = action\n","        self.rewards[self.index] = reward\n","        self.next_states[self.index] = next_state\n","        self.dones[self.index] = done\n","        \n","        self.index = (self.index + 1) % self.max_length\n","        if self.size < self.max_length:\n","            self.size = self.index\n","            \n","    def sample(self, batch_size):\n","        \"\"\"Retorna um batch de experi√™ncias.\n","        \n","        Par√¢metros\n","        ----------\n","        batch_size: int\n","            Tamanho do batch de experi√™ncias.\n","\n","        Retorna\n","        -------\n","        states: np.array\n","            Batch de estados.\n","        actions: np.array\n","            Batch de a√ß√µes.\n","        rewards: np.array\n","            Batch de recompensas.\n","        next_states: np.array\n","            Batch de estados seguintes.\n","        dones: np.array\n","            Batch de flags indicando se o epis√≥dio acabou.\n","        \"\"\"\n","        # Escolhe √≠ndices aleatoriamente do Replay Buffer\n","        idxs = np.random.randint(0, self.size, size=batch_size)\n","\n","        return (self.states[idxs], self.actions[idxs], self.rewards[idxs], self.next_states[idxs], self.dones[idxs])"],"execution_count":null,"outputs":[]}]}