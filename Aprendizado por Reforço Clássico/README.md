# ğŸ° Aprendizado por ReforÃ§o ClÃ¡ssico

## [ğŸ° Multi-armed Bandits](Bandits)

Um dos problemas mais clÃ¡ssicos e simples em aprendizado por reforÃ§o Ã© o problema do *k*-armed Bandit (em portuguÃªs, Roleta de *k*-alavancas). Nele uma inteligÃªncia artificial iria para um cassino e encontraria uma roleta com *k* alavancas, lÃ¡ ela teria que aprender - por meio de aprendizado por reforÃ§o - a escolher a alavanca que lhe da mais dinheiro.

## [ğŸ” MÃ©todos de Monte Carlo](Monte%20Carlo)

Os mÃ©todos de **Monte Carlo** sÃ£o algoritmos de Aprendizado por ReforÃ§o que estimam as funÃ§Ãµes de valor com base em suas *experiÃªncias*, obtidas atravÃ©s da interaÃ§Ã£o com o ambiente. Nesses mÃ©todos, os valores sÃ£o obtidos a partir do cÃ¡lculo da mÃ©dia dos retornos de cada episÃ³dio.

## [ğŸ“… Temporal-Difference Learning](Temporal-Difference)
MÃ©todos de **Temporal-Difference** sÃ£o algoritmos de Aprendizado por ReforÃ§o que aprendem utilizando principalmente tÃ©cnicas de *bootstrapping* (uma estimativa a base de amostragem) da sua funÃ§Ã£o de valor. Esses mÃ©todos amostram do ambiente (assim como mÃ©todos de Monte Carlo), mas fazem atualizaÃ§Ãµes com base em estimativas atuais. Ou seja, enquanto mÃ©todos de Monte Carlo atualizam apenas quando a saÃ­da final jÃ¡ Ã© sabida, **Temporal-Difference** atualiza as prediÃ§Ãµes para representarem valores mais tardios sobre o futuro antes do resultado final jÃ¡ ser conhecido.