# üé∞ Aprendizado por Refor√ßo Cl√°ssico

## [üé∞ Multi-armed Bandits](Bandits)

Um dos problemas mais cl√°ssicos e simples em aprendizado por refor√ßo √© o problema do *k*-armed Bandit (em portugu√™s, Roleta de *k*-alavancas). Nele uma intelig√™ncia artificial iria para um cassino e encontraria uma roleta com *k* alavancas, l√° ela teria que aprender - por meio de aprendizado por refor√ßo - a escolher a alavanca que lhe da mais dinheiro.

## [üèî M√©todos de Monte Carlo](Monte%20Carlo)

Os m√©todos de **Monte Carlo** s√£o algoritmos de Aprendizado por Refor√ßo que estimam as fun√ß√µes de valor com base em suas *experi√™ncias*, obtidas atrav√©s da intera√ß√£o com o ambiente. Nesses m√©todos, os valores s√£o obtidos a partir do c√°lculo da m√©dia dos retornos de cada epis√≥dio.

## [üìÖ Temporal-Difference Learning](Temporal-Difference)
M√©todos de **Temporal-Difference** s√£o algoritmos de Aprendizado por Refor√ßo que aprendem utilizando principalmente t√©cnicas de *bootstrapping* (uma estimativa a base de amostragem) da sua fun√ß√£o de valor. Esses m√©todos amostram do ambiente, assim como m√©todos de Monte Carlo, mas fazem atualiza√ß√µes com base em estimativas intermedi√°rias. Ou seja, enquanto m√©todos de Monte Carlo atualizam as suas predi√ß√µes apenas no fim do epis√≥dio (quando todos os retornos j√° s√£o conhecidos), m√©todos de  **Temporal-Difference** atualizam as suas predi√ß√µes a cada instante de tempo, utilizando *bootstrapping* para estimar o retorno do epis√≥dio.