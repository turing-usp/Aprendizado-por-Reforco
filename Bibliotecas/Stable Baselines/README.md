# üëæ Stable Baselines

 Guia de como utilizar a biblioteca [Stable Baselines](https://github.com/DLR-RM/stable-baselines3) para projetos de Aprendizado por Refor√ßo.

O guia completo encontra-se neste notebook:

### [Guia Completo em Notebook](Stable%20Baselines.ipynb)

### [Guia Completo no Google Colaboratory](https://colab.research.google.com/github/turing-usp/Aprendizado-por-Reforco/blob/main/Bibliotecas/Stable%20Baselines/Stable%20Baselines.ipynb)

Entretanto, caso voc√™ n√£o queira ver os exemplos em c√≥digo, o texto do guia est√° presente a seguir.

## √çndice

- [üëæ Stable Baselines](#-stable-baselines)
  - [O que √© Stable Baselines?](#o-que-√©-stable-baselines)
  - [Instala√ß√£o](#instala√ß√£o)
  - [Como usar Stable Baselines?](#Como-usar-Stable-Baselines)
    - [Gym](#Gym)
    - [O que √© um Ambiente?](#O-que-√©-um-Ambiente)
    - [Como Funciona um Ambiente do Gym?](#Como-Funciona-um-Ambiente-do-Gym)
    - [Criando um Ambiente](#Criando-um-Ambiente)
    - [Criando um Agente](#Criando-um-Agente)
    - [Rodando um Epis√≥dio](#Rodando-um-Epis√≥dio)
    - [Treinamento](#Treinamento)
    - [Monitorando o Treinamento](#Monitorando-o-Treinamento)

## O que √© Stable Baselines?

A **Stable Baselines** √© uma biblioteca de Aprendizado por Refor√ßo que implementa diversos algoritmos de agentes de RL, al√©m de v√°rias funcionalidades √∫teis para o treinamento de um agente. Suas implementa√ß√µes s√£o bem simples e intuitivas, mas sem deixarem de ser otimizadas e poderosas, buscando facilitar o desenvolvimento de projetos de Aprendizado por Refor√ßo de alta qualidade.

![Logo](https://github.com/hill-a/stable-baselines/raw/master/docs//_static/img/logo.png "Logo da Stable Baselines")

## Instala√ß√£o

Para instalar a biblioteca com o `pip`, basta rodar:

```
pip install stable-baselines3
```

OBS: √â necess√°rio instalar antes o PyTorch.

## Como usar Stable Baselines?

Com Stable Baselines, o processo de criar e treinar um agente √© bem simples. Entretanto, caso voc√™ n√£o saiba muito de Aprendizado por Refor√ßo, √© primeiro preciso passar por alguns conhecimentos b√°sicos.

### Gym

O **[Gym](https://gym.openai.com/)** √© uma biblioteca desenvolvida pela OpenAI que cont√©m v√°rias implementa√ß√µes prontas de ambientes de Aprendizado por Refor√ßo. Ela √© muito utilizada quando se quer testar um algoritmo de agente sem ter o trabalho de programar seu pr√≥prio ambiente.

<img src="https://camo.githubusercontent.com/25043fb622d3f9115a263fb71c61adb08c1d7790/68747470733a2f2f7072617665656e702e636f6d2f70726f6a656374732f484f4941574f472f6f75747075742e676966" alt="Exemplos de Ambientes do Gym" class="inline"/>

### O que √© um Ambiente?

Um **Ambiente** de Aprendizado por Refor√ßo √© um espa√ßo que representa o nosso problema, √© o objeto com o qual o nosso agente deve interagir para cumprir sua fun√ß√£o. Isso significa que o agente toma **a√ß√µes** nesse ambiente, e recebe **recompensas** dele com base na qualidade de sua tomada de decis√µes.

Todos os ambientes s√£o dotados de um **espa√ßo de observa√ß√µes**, que √© a forma pela qual o agente recebe informa√ß√µes e deve se basear para a tomada de decis√µes, e um **espa√ßo de a√ß√µes**, que especifica as a√ß√µes poss√≠veis do agente. No xadrez, por exemplo, o espa√ßo de observa√ß√µes seria o conjunto de todas as configura√ß√µes diferentes do tabuleiro, e o espa√ßo de a√ß√µes seria o conjunto de todos os movimentos permitidos.

<img src="https://www.raspberrypi.org/wp-content/uploads/2016/08/giphy-1-1.gif" alt="Uma A√ß√£o do Xadrez" class="inline"/>

### Como Funciona um Ambiente do Gym?

Agora que voc√™ j√° sabe o que √© um ambiente, √© preciso entender como nosso agente interage efetivamente com ele. Todos os ambientes do Gym possuem alguns m√©todos simples para facilitar a comunica√ß√£o com eles:

<br>

| M√©todo               | Funcionalidade                                          |
| :------------------- |:------------------------------------------------------- |
| `reset()`              | Inicializa o ambiente e recebe a observa√ß√£o inicial     |
| `step(action)`         | Executa uma a√ß√£o e recebe a observa√ß√£o e a recompensa   |
| `render()`             | Renderiza o ambiente                                    |
| `close()`              | Fecha o ambiente                                        |

<br>

Assim, o c√≥digo para interagir com o ambiente costuma seguir o seguinte modelo:

---

```python
ambiente = gym.make("Nome do Ambiente")                         # Cria o ambiente
observa√ß√£o = ambiente.reset()                                   # Inicializa o ambiente
acabou = False

while not acabou:
    ambiente.render()                                           # Renderiza o ambiente
    observa√ß√£o, recompensa, acabou, info = ambiente.step(a√ß√£o)  # Executa uma a√ß√£o
    
ambiente.close()                                                # Fecha o ambiente
```

---

### Criando um Ambiente

Para utilizar um dos ambientes do Gym, n√≥s utilizamos a fun√ß√£o ```gym.make()```, passando o nome do ambiente desejado como par√¢metro e guardando seu valor retornado em uma vari√°vel que chamaramos de ```env```. A lista com todos os ambiente pode ser encontrada [aqui](https://gym.openai.com/envs/#classic_control).

```python
env = gym.make("CartPole-v1")
```

#### Exemplo - CartPole

Para exemplificar, vamos pensar no ambiente ```CartPole-v1```, um ambiente bem simples que modela um p√™ndulo invertido em cima de um carrinho buscando seu estado de equil√≠brio.

<img src="https://miro.medium.com/max/1200/1*jLj9SYWI7e6RElIsI3DFjg.gif" width="400px" alt="Ambiente do CartPole-v1" class="inline"/>

Antes de treinar qualquer agente, primeiro √© preciso entender melhor quais as caracter√≠sticas do nosso ambiente.

O **Espa√ßo de Observa√ß√£o** do CartPole √© definido por 4 informa√ß√µes:

<br>

|     | Informa√ß√£o                         | Min     | Max    |
| :-- | :--------------------------------- | :-----: | :----: |
| 0   | Posi√ß√£o do Carrinho                | -4.8    | 4.8    |
| 1   | Velocidade do Carrinho             | -Inf    | Inf    |
| 2   | √Çngulo da Barra                    | -24 deg | 24 deg |
| 3   | Velocidade na Extremidade da Barra | -Inf    | Inf    |

<br>

Dessa forma, a cada instante recebemos uma lista da observa√ß√£o com o seguinte formato:

```python
[-3.3715708e+00 -2.6997593e+38  2.7833584e-01  2.0276438e+38]
```

J√° o **Espa√ßo de A√ß√£o** √© composto por duas a√ß√µes √∫nicas: mover o carrinho para a **esquerda** ou para a **direita**.

Quando queremos mover o carrinho para a esquerda, fazemos um `env.step(0)`; quando queremos mov√™-lo para a direita, enviamos um `env.step(1)`

### Criando um Agente

Depois de escolhermos nosso ambiente, j√° podemos pensar em qual algoritmo de agente queremos usar.

A biblioteca disponibiliza algoritmos de diversos tipos, como *Policy Gradients*, *Actor Critics*, *DQN*, etc. Nem todos eles suportam todos os tipos de ambientes, ent√£o √© recomend√°vel dar uma olhada na [p√°gina oficial dos algoritmos](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html).

#### Inicializa√ß√£o

Todos os algoritmos s√£o inicializados de uma forma parecida, n√≥s instanciamos eles com alguns par√¢metros em comum: ```policy```, que define a arquitetura da rede neural e ```env```, que define o ambiente no qual o agente vai treinar. Assim, a inicializa√ß√£o segue o seguinte formato:

```python
agente = ALGORITMO(policy, env)
```

Como exemplo, vamos criar um **PPO**, um tipo de Actor-Critic:

```python
from stable_baselines import PPO

model = PPO('MlpPolicy', env, seed=1, verbose=1)
```

Todos os agentes tamb√©m possuem alguns m√©todos em comuns bem importantes de se conhecer:

<br>

| M√©todo        | Funcionalidade                          |
| :------------ |:--------------------------------------- |
| `learn()`       | Treina o agente                         |
| `predict(obs)`  | Escolhe uma a√ß√£o com base na observa√ß√£o |
| `save(caminho)` | Salve o agente                          |
| `load(caminho)` | Carrega o agente                        |

<br>

Dessa forma, se quisermos escolher a pr√≥xima a√ß√£o do nosso agente, n√≥s rodamos:

```python
agente.predict(observa√ß√£o)
```

### Rodando um Epis√≥dio

Bom, agora que j√° temos nosso agente e nosso ambiente, j√° podemos rodar nosso primeiro epis√≥dio!

Para isso, vamos criar uma fun√ß√£o `run_episode` para simplificar o processo:

```python
# A fun√ß√£o recebe o ambiente e o agente como par√¢metros
def run_episode(env, model, render=False):
    # Primeiro, inicializamos o ambiente e guardamos a observa√ß√£o inicial em 'obs'
    obs = env.reset()

    # Guarda se o epis√≥dio terminou ou n√£o
    done = False
    
    # Loop do epis√≥dio
    while not done:
        # Nosso modelo prediz a a√ß√£o 'action' a ser tomada com base na nossa observa√ß√£o 'obs'
        action, _states = model.predict(obs)
        
        # Tomamos a a√ß√£o 'action', e recebemos uma nova observa√ß√£o 'obs', uma recompensa 'reward'
        # e se o epis√≥dio terminou 'done'
        obs, reward, done, info = env.step(action)
        
        # Renderiza o ambiente, caso desejado
        if render:
            env.render()
            
        # Finaliza o epis√≥dio, caso tenha terminado
        if done:
            break
    
    # Quando terminado, fechamos o ambiente
    env.close()
```

Para rodar o epis√≥dio, basta fazer:

```python
run_episode(env, model, render=True)
```

Se voc√™ tentou rodar um epis√≥dio, provavelmente o resultado n√£o foi t√£o bom assim. Isso √© porque precisamos treinar nosso agente para que ele saiba as melhores a√ß√µes a se tomar.

### Avaliando o Agente

Para melhor avaliar o desempenho do nosso agente, podemos utilizar a fun√ß√£o `evaluate_policy` da biblioteca, que roda uma quantidade determin√°vel de epis√≥dios e retorna a recompensa m√©dia obtida.

```python
# Ambiente separado para avalia√ß√£o
eval_env = gym.make('CartPole-v1')

# Avaliando o agente
mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=25, deterministic=True)
```

### Treinamento

O treinamento do agente acontece de maneira bem simples, basta rodar o m√©todo `.learn()` com a quantidade de instantes de tempo `total_timesteps` que desejamos treinar.

Ao longo do treinamento, nosso agente vai mostrando algumas informa√ß√µes importantes no ouput, como a dura√ß√£o m√©dia dos epis√≥dios `mean_episode_length`, a entropia `entropy`, o custo da fun√ß√£o de custo `loss`, etc.

```python
model.learn(total_timesteps=20020)
```

Depois de treinarmos o nosso agente, podemos rodar mais um epis√≥dio para ver como ele melhorou.

```python
run_episode(env, model, render=True)
```

Esse √© todo o conhecimento necess√°rio para resolver mais ambientes simples. Entretanto, ainda existem v√°rias outras funcionalidades muito interessantes da biblioteca que valem a pena aprender.

### Monitorando o Treinamento

Para obter mais informa√ß√µes do treinamento, podemos utilizar o *wrapper* `Monitor` da biblioteca para monitorar o desempenho do nosso agente mesmo durante o treino.

Para isto, primeiro devemos criar uma pasta de logs:

```python
import os

# Cria um diret√≥rio de logs
log_dir = "./logs/"
os.makedirs(log_dir, exist_ok=True)
```

Em seguida, criamos o nosso ambiente e passamos ele para o nosso *wrapper*:

```python
from stable_baselines3.common.monitor import Monitor

# Cria o ambiente
env = gym.make('CartPole-v1')

# Encapsula ele no wrapper Monitor
env = Monitor(env, log_dir)
```

A partir da√≠, basta treinar o nosso agente como normal, utilizando o nosso novo ambiente encapsulado pelo *wrapper*:

```python
model = PPO('MlpPolicy', env, seed=1, verbose=1).learn(total_timesteps=40000)
```

Terminado o treinamento, podemos plotar os resultados utilizando o `result_plotter`:

```python
from stable_baselines3.common import results_plotter

# Plota os resultados
results_plotter.plot_results([log_dir], 1e5, results_plotter.X_TIMESTEPS, "PPO CartPole")
```

![](img/grafico.png)

Pronto! Agora temos uma visualiza√ß√£o da performance do nosso agente durante o seu treinamento. Entretanto, podemos criar a nossa pr√≥pria fun√ß√£o de visualiza√ß√£o para melhor analisar o modelo:

```python
import matplotlib.pyplot as plt

def plot_results(log_folder, window=10, title='Curva de Aprendizado'):
    """
    Plota os resultados.

    :param log_folder: (str) diret√≥rio dos resultados a serem plotados
    :param window: (int) tamanho da janela da m√©dia m√≥vel
    :param title: (str) t√≠tulo do plot
    """
    # Obt√©m os resultados
    results = results_plotter.load_results(log_folder)
    x, y = results_plotter.ts2xy(results, 'timesteps')
    
    # Calcula a m√©dia m√≥vel do retorno
    y_smoothed = results_plotter.rolling_window(y, window=window).mean(axis=1)
    
    # Plota os resultados
    fig = plt.figure(title, figsize=(10, 5))
    plt.scatter(x, y, s=2)
    plt.plot(x[window-1:], y_smoothed, color='darkblue', label="M√©dia M√≥vel")
    plt.xlabel('Timesteps')
    plt.ylabel('Retorno M√©dio')
    plt.title(title)
    plt.legend()
    plt.show()
```

Agora conseguimos visualizar melhor o treinamento:

```python
plot_results(log_dir)
```

![](img/rolling.png)
