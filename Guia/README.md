# üó∫Ô∏è Guia de Aprendizado

Boas vindas ao **üó∫Ô∏è Guia de Aprendizado** do Reposit√≥rio de Aprendizado por Refor√ßo! Aqui, voc√™ encontra sugest√µes de como seguir os t√≥picos do reposit√≥rio de modo a organizar melhor seu aprendizado.

## üíØ T√≥picos Introdut√≥rios

- **[Introdu√ß√£o ao Aprendizado por Refor√ßo](../Introdu√ß√£o)**
  - Explica os conceitos b√°sicos da √°rea de Aprendizado por Refor√ßo.
  - Pr√©-requisitos: Nenhum.
  
## üë®‚Äçüè≠ Introdu√ß√µes Pr√°ticas

Antes de estudar alguns algoritmos de Aprendizado por Refor√ßo, pode ser interessante se familiarizar com algumas bibliotecas da √°rea de maneira pr√°tica, de modo a motivar os estudos.

- **[Gym](../Bibliotecas/Gym)**
  - Aprenda como funciona um ambiente na biblioteca do Gym.
  - Pr√©-requisitos: [Introdu√ß√£o ao Aprendizado por Refor√ßo](../Introdu√ß√£o).

- **[Stable Baselines](../Bibliotecas/Stable%20Baselines)**
  - Aprenda na pr√°tica a programar um agente de Aprendizado por Refor√ßo Profundo.
  - Pr√©-requisitos: [Gym](../Bibliotecas/Gym).

## üé∞ Aprendizado por Refor√ßo Cl√°ssico

Os algoritmos fundamentais do Aprendizado por Refor√ßo se encontram nesta categoria. √â importante entender pelo menos alguns para adentrar assuntos mais modernos da √°rea. A seguir, est√° apresentada uma taxonomia desses algoritmos cl√°ssicos:

<p align="center">
  <br/>
  <img src="../img/taxonomia_classico.png" alt="Taxonomia dos Algoritmos de Aprendizado por Refor√ßo Cl√°ssico"/>
</p>

- **[Bandits](../Aprendizado%20por%20Refor√ßo%20Cl√°ssico/Bandits)**
  - O problema mais cl√°ssico da √°rea! Aprender a escolher a melhor a√ß√£o em uma situa√ß√£o simples.
  - Pr√©-requisitos: [Introdu√ß√£o ao Aprendizado por Refor√ßo](../Introdu√ß√£o).

- **[Monte Carlo](../Aprendizado%20por%20Refor√ßo%20Cl√°ssico/Monte%20Carlo)**
  - Um simples m√©todo de Aprendizado por Refor√ßo para resolver problemas com mais estados que os Bandits.
  - Pr√©-requisitos: [Bandits](../Aprendizado%20por%20Refor√ßo%20Cl√°ssico/Bandits).
  
- **[Q-Learning](../Aprendizado%20por%20Refor√ßo%20Cl√°ssico/Temporal-Difference/Q-Learning)**
  - Um dos algoritmo mais importantes de Aprendizado por Refor√ßo! Mais aplic√°vel que Monte Carlo e Bandits.
  - Pr√©-requisitos: [Bandits](../Aprendizado%20por%20Refor√ßo%20Cl√°ssico/Bandits).
  
## üß† Aprendizado por Refor√ßo Profundo

Por fim, temos os algoritmos de Aprendizado por Refor√ßo Profundo, que se baseiam em redes neurais. Estes s√£o alguns dos mais utilizados em aplica√ß√µes modernas da √°rea, j√° que podem representar problemas mais complexos. A seguir, est√° a apresentada uma taxonomia desses algoritmos:

<p align="center">
  <br/>
  <img src="../img/taxonomia_profundo.png" alt="Taxonomia dos Algoritmos de Aprendizado por Refor√ßo Profundo"/>
</p>

- **[Deep Q-Learning](../Aprendizado%20por%20Refor√ßo%20Profundo/Deep%20Q-Learning)**
  - O algoritmo mais popular de Aprendizado por Refor√ßo Profundo. Um aprimoramento de Q-Learning com redes neurais.
  - Pr√©-requisitos: [Q-Learning](../Aprendizado%20por%20Refor√ßo%20Cl√°ssico/Temporal-Difference/Q-Learning).

- **[Policy Gradient](../Aprendizado%20por%20Refor√ßo%20Profundo/Policy%20Gradient)**
  - Um importante algoritmo de Aprendizado por Refor√ßo profundo que aproxima diretamente a pol√≠tica √≥tima, sem estimar a fun√ß√£o de valor.
  - Pr√©-requisitos: [Monte Carlo](../Aprendizado%20por%20Refor√ßo%20Cl√°ssico/Monte%20Carlo).
  
- **[Actor-Critic](../Aprendizado%20por%20Refor√ßo%20Profundo/Actor-Critic)**
  - Algoritmo resultante da poderosa combina√ß√£o entre Policy Gradient e uma rede neural para estimar a fun√ß√£o de valor.
  - Pr√©-requisitos: [Deep Q-Learning](../Aprendizado%20por%20Refor√ßo%20Cl√°ssico/Temporal-Difference/Q-Learning) e [Policy Gradient](../Aprendizado%20por%20Refor√ßo%20Profundo/Policy%20Gradient).
